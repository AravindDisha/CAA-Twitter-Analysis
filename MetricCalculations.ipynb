{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AravindDisha/CAA-Twitter-Analysis/blob/main/MetricCalculations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv_c0FqqXfQn"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_oYxxeLsl3k",
        "outputId": "28b9e40d-fb79-4b3e-c61f-d063a67ece1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!pip install botometer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: botometer in /usr/local/lib/python3.6/dist-packages (1.4)\n",
            "Requirement already satisfied: tweepy>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from botometer) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from botometer) (2.21.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.5.0->botometer) (1.7.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.5.0->botometer) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.5.0->botometer) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->botometer) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->botometer) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->botometer) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->botometer) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.5.0->botometer) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0RsRHCBXczr",
        "outputId": "1e7574f4-a588-4dc7-e2c6-6c2040dfad11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "!pip install twint                                  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: twint in /usr/local/lib/python3.6/dist-packages (2.1.19)\n",
            "Requirement already satisfied: aiodns in /usr/local/lib/python3.6/dist-packages (from twint) (2.0.0)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.6/dist-packages (from twint) (1.17.0)\n",
            "Requirement already satisfied: aiohttp-socks in /usr/local/lib/python3.6/dist-packages (from twint) (0.3.9)\n",
            "Requirement already satisfied: cchardet in /usr/local/lib/python3.6/dist-packages (from twint) (2.1.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from twint) (1.0.3)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.6/dist-packages (from twint) (0.1.11)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from twint) (3.6.2)\n",
            "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.6/dist-packages (from twint) (7.6.0)\n",
            "Requirement already satisfied: googletransx in /usr/local/lib/python3.6/dist-packages (from twint) (2.4.2)\n",
            "Requirement already satisfied: pysocks in /usr/local/lib/python3.6/dist-packages (from twint) (1.7.1)\n",
            "Requirement already satisfied: schedule in /usr/local/lib/python3.6/dist-packages (from twint) (0.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from twint) (4.6.3)\n",
            "Requirement already satisfied: pycares>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from aiodns->twint) (3.1.1)\n",
            "Requirement already satisfied: typing; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiodns->twint) (3.6.6)\n",
            "Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.6/dist-packages (from geopy->twint) (1.50)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp-socks->twint) (19.3.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (1.18.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (1.4.2)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.6.6)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (1.1.0)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (4.7.5)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.0.4)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch->twint) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletransx->twint) (2.21.0)\n",
            "Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pycares>=3.0.0->aiodns->twint) (1.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->twint) (1.12.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletransx->twint) (2020.4.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.5.0->pycares>=3.0.0->aiodns->twint) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxLg8HKQXlZU",
        "outputId": "ae70347d-dbf4-4cfc-cca5-bcf50c3f9894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install googletrans"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletrans) (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU0L1XTWIwtz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#mount on other drive account for more space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGQdHh9BKu26",
        "outputId": "d81dc2de-fe06-4657-98c7-88cfc15e0332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import IPython\n",
        "import ast\n",
        "import csv\n",
        "import datetime\n",
        "import json\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics \n",
        "from statistics import mode \n",
        "import networkx as nx\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "\n",
        "import numpy as np\n",
        "import os.path\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "import requests\n",
        "import string\n",
        "import sys\n",
        "import time\n",
        "import twint\n",
        "from IPython.display import HTML,display\n",
        "from datetime import timedelta\n",
        "from googletrans import Translator\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Flatten, Conv1D, SpatialDropout1D, MaxPooling1D,AveragePooling1D, Bidirectional, merge, concatenate, Input, Dropout, LSTM\n",
        "from keras.models import Sequential, Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from operator import itemgetter\n",
        "from os import listdir\n",
        "from os import path\n",
        "from os.path import isfile, join\n",
        "from pprint import pprint\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.externals.six import StringIO\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_validate as cross_validation, ShuffleSplit, cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "# import warnings\n",
        "translator = Translator()\n",
        "# warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISXIEcShYaqJ"
      },
      "source": [
        "import tweepy as tw\n",
        "#Add your access codes here if you get yours. Only 320 requests are allowed per day I think\n",
        "consumer_key= ~\n",
        "consumer_secret= ~\n",
        "access_token= ~\n",
        "access_token_secret= ~\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZNyaqfj74Jd"
      },
      "source": [
        "# For Representation\n",
        "class Tweet(object):\n",
        "    def __init__(self, s, embed_str=False):\n",
        "        if not embed_str:\n",
        "            # Use Twitter's oEmbed API\n",
        "            # https://dev.twitter.com/web/embedded-tweets\n",
        "            api = 'https://publish.twitter.com/oembed?url={}'.format(s)\n",
        "            response = requests.get(api)\n",
        "            self.text = response.json()[\"html\"]\n",
        "        else:\n",
        "            self.text = s\n",
        "\n",
        "    def _repr_html_(self):\n",
        "        return self.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNp3CMIsYcPp"
      },
      "source": [
        "col_names = ['id','conversation_id',\t'created_at',\t'date',\t'time',\t'timezone',\t'user_id',\t'username',\t'name',\t'place',\t'tweet',\t'mentions',\t'urls',\t'photos',\t'replies_count',\t'retweets_count',\t'likes_count',\t'hashtags',\t'cashtags',\t'link',\t'retweet',\t'quote_url',\t'video',\t'near',\t'geo',\t'source',\t'user_rt_id',\t'user_rt',\t'retweet_id',\t'reply_to',\t'retweet_date',\t'translate',\t'trans_src',\t'trans_dest']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6GK5rMrBmT2"
      },
      "source": [
        "# Load Stance Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwxKN6ZxBw3j"
      },
      "source": [
        "# All models for stance detection\n",
        "filenames = [\"/content/drive/My Drive/stance_data/models/Support_Vector_Machine_1.pkl\",\"/content/drive/My Drive/stance_data/models/Random_Forest_Classifier_1.pkl\",\"/content/drive/My Drive/stance_data/models/Gradient_Boosting_Classifier_1.pkl\",\"/content/drive/My Drive/stance_data/models/Logistic_Regression_1.pkl\",\"/content/drive/My Drive/stance_data/models/Neural_Network_1.pkl\",\"/content/drive/My Drive/stance_data/models/Gaussian_NB_1.pkl\",\"/content/drive/My Drive/stance_data/models/K_Neighbors_Classifier_1.pkl\",\"/content/drive/My Drive/stance_data/models/Decision_Tree_Classifier_1.pkl\",\"/content/drive/My Drive/stance_data/models/Support_Vector_Machine_2.pkl\",\"/content/drive/My Drive/stance_data/models/Random_Forest_Classifier_2.pkl\",\"/content/drive/My Drive/stance_data/models/Gradient_Boosting_Classifier_2.pkl\",\"/content/drive/My Drive/stance_data/models/Logistic_Regression_2.pkl\",\"/content/drive/My Drive/stance_data/models/Neural_Network_2.pkl\",\"/content/drive/My Drive/stance_data/models/Gaussian_NB_2.pkl\",\"/content/drive/My Drive/stance_data/models/K_Neighbors_Classifier_2.pkl\",\"/content/drive/My Drive/stance_data/models/Decision_Tree_Classifier_2.pkl\",\"/content/drive/My Drive/stance_data/models/lstm1_3.pkl\",\"/content/drive/My Drive/stance_data/models/lstm2_3.pkl\",\"/content/drive/My Drive/stance_data/models/lstm1_4.pkl\",\"/content/drive/My Drive/stance_data/models/lstm2_4.pkl\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "f4fcac4e-52f0-4a07-f771-8a129adbcb98",
        "id": "yqTcSV5m-7J6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# models being used currently\n",
        "print('\\n'.join(filenames[2:5]))\n",
        "print(filenames[17])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/stance_data/models/Gradient_Boosting_Classifier_1.pkl\n",
            "/content/drive/My Drive/stance_data/models/Logistic_Regression_1.pkl\n",
            "/content/drive/My Drive/stance_data/models/Neural_Network_1.pkl\n",
            "/content/drive/My Drive/stance_data/models/lstm2_3.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jJZoJa9Bz6X",
        "outputId": "e2dae33e-0790-402a-c4fa-25186f3a3fc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Load from file\n",
        "models1 = {}\n",
        "for pkl_filename in filenames[:8]:\n",
        "  with open(pkl_filename, 'rb') as file:\n",
        "      pickle_model = pickle.load(file)\n",
        "      # print(pickle_model)\n",
        "      models1[pkl_filename] = pickle_model\n",
        "\n",
        "models2 = {}\n",
        "for pkl_filename in filenames[8:16]:\n",
        "  with open(pkl_filename, 'rb') as file:\n",
        "      pickle_model = pickle.load(file)\n",
        "      models2[pkl_filename] = pickle_model\n",
        "\n",
        "models3 = {}\n",
        "for pkl_filename in filenames[16:18]:\n",
        "  with open(pkl_filename, 'rb') as file:\n",
        "      pickle_model = pickle.load(file)\n",
        "      models3[pkl_filename] = pickle_model\n",
        "\n",
        "models4 = {}\n",
        "for pkl_filename in filenames[18:]:\n",
        "  with open(pkl_filename, 'rb') as file:\n",
        "      pickle_model = pickle.load(file)\n",
        "      models4[pkl_filename] = pickle_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rav2kX56B9qX",
        "outputId": "43a0f0b7-7b84-4f9b-b37f-374dbb1524f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Functions from the stance models to help predict the stance\n",
        "def labelStance(labelDict, data):\n",
        "\tfor key, val in labelDict.items():\n",
        "\t\tdata.loc[data[\"Stance\"] == val, \"Stance\"] = int(key)\n",
        "\treturn data\n",
        "\n",
        "def readGlobalVecData(glove_word_vec_file):\n",
        "\tfile = open(glove_word_vec_file, encoding=\"utf8\")\n",
        "\trawData = file.readlines()\n",
        "\tglove_word_vec_dict = {}\n",
        "\tfor line in rawData:\n",
        "\t\tline = line.strip().split()\n",
        "\t\ttag = line[0]\n",
        "\t\tvec = line[1:]\n",
        "\t\tglove_word_vec_dict[tag] = np.array(vec, dtype=float)\n",
        "\treturn glove_word_vec_dict\n",
        "\n",
        "def getWordVector(word, glove_word_vec_dict):\n",
        "\tif word in glove_word_vec_dict:\n",
        "\t\treturn glove_word_vec_dict[word]\n",
        "\treturn np.zeros_like(glove_word_vec_dict[\"dummy\"])\n",
        "\n",
        "def sumVectors(finalList, glove_word_vec_dict):\n",
        "\tnumNonZero = 0\n",
        "\tvector = np.zeros_like(glove_word_vec_dict[\"dummy\"])\n",
        "\tfor word in finalList:\n",
        "\t\tvect = getWordVector(word,glove_word_vec_dict)\n",
        "\t\tif vect.sum() != 0:\n",
        "\t\t\tvector += vect\n",
        "\t\t\tnumNonZero += 1\n",
        "\tif numNonZero:\n",
        "\t\tvector = vector/numNonZero\n",
        "\treturn vector\n",
        "\n",
        "def simplify(word):\n",
        "\tdump = ''\n",
        "\ttemp = []\n",
        "\tlistOfWords = list(filter(None,re.split(\"([A-Z][^A-Z]*)\",word)))\n",
        "\tif len(listOfWords) == len(word):\n",
        "\t\treturn word.lower()\n",
        "\tfor i in range(len(listOfWords)):\n",
        "\t\tlistOfWords[i] = listOfWords[i].lower()\n",
        "\t\tif len(listOfWords[i]) == 1:\n",
        "\t\t\tdump = dump + listOfWords[i]\n",
        "\t\t\tif dump in words.words() and len(dump) > 2:\n",
        "\t\t\t\ttemp.append(dump)\n",
        "\t\t\t\tdump = ''\n",
        "\t\telse:\n",
        "\t\t\ttemp.append(listOfWords[i])\n",
        "\treturn temp\n",
        "\n",
        "def glove_test(glove_word_vec_dict, trainTweets):\n",
        "    def createTokens(data,glove_word_vec_dict):\n",
        "        # listOfTweets = []\n",
        "        # listOfStances = []\n",
        "        tweetVector = []\n",
        "        for text in data:\n",
        "            # Create a sentence using target and the tweet. Word vector will be formed from this.\n",
        "            example_sentence = 'CAA' +' '+ text\n",
        "            # Remove punctuation\n",
        "            final_sentence = example_sentence.translate(string.punctuation)\n",
        "            wordList = word_tokenize(final_sentence)\n",
        "            finalList = []\n",
        "            s = ' '.join([i for i in wordList if i.isalpha()])\n",
        "            # create tokens from the string and stem them\n",
        "            wordList = word_tokenize(s)\n",
        "            wordList = [w.lower() for w in wordList]\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            wordList = [w for w in wordList if not w in stop_words]\n",
        "            for word in wordList:\n",
        "                #to break any combined word into its components for eg, hashtags\n",
        "                finalList += simplify(word)\n",
        "            final_sentence = ' '.join(finalList)\n",
        "            # listOfTweets.append(final_sentence)\n",
        "            # listOfStances.append(row[\"Stance\"])\n",
        "            tweetVector.append(sumVectors(finalList,glove_word_vec_dict))\n",
        "        return tweetVector\n",
        "    # Remove punctuation from and tokenize the tweets\n",
        "    testTweetVector = createTokens(trainTweets, glove_word_vec_dict)\n",
        "    return testTweetVector\n",
        "\n",
        "gloveFile = \"/content/drive/My Drive/stance_data/glove/glove.twitter.27B.200d.txt\"\n",
        "\n",
        "print(\"\\nLoading Glove data in progress...\")\n",
        "glove_word_vec_dict = readGlobalVecData(gloveFile)\n",
        "print(\"\\nLoading Glove data is done...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading Glove data in progress...\n",
            "\n",
            "Loading Glove data is done...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkqjxZA2DHuz",
        "outputId": "fcacdd7a-557d-4d06-b68f-18d7339280f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Functions from the stance models to help predict the stance \n",
        "file = \"/content/drive/My Drive/stance_data/demon/csv_files/demoapril.csv\"\n",
        "df = pd.read_csv(file,encoding='utf-8')\n",
        "df.info()\n",
        "df = df.reset_index(drop=True)\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = text.replace('x', '')\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text\n",
        "\n",
        "df['text'] = df.text + ' ' + 'Demonitization'\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "df['text'] = df['text'].str.replace('\\d+', '')\n",
        "\n",
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(df['text'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "tokenizer1 = tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2817 entries, 0 to 2816\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   ID      2817 non-null   object\n",
            " 1   text    2814 non-null   object\n",
            " 2   Stance  2811 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 66.1+ KB\n",
            "Found 6540 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as48Wz3IDI6h"
      },
      "source": [
        "# training = \"/content/drive/My Drive/stance_data/semval/training.txt\"\n",
        "# df = pd.read_csv(training, sep='\\t',header=0,encoding='utf-8')\n",
        "# df.info()\n",
        "# df = df.reset_index(drop=True)\n",
        "# REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "# BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "# STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# def clean_text(text):\n",
        "#     text = text.lower() # lowercase text\n",
        "#     text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "#     text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "#     text = text.replace('x', '')\n",
        "#     text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "#     return text\n",
        "\n",
        "# df['Tweet'] = df.Tweet + ' ' + df.Target\n",
        "# df['Tweet'] = df['Tweet'].apply(clean_text)\n",
        "# df['Tweet'] = df['Tweet'].str.replace('\\d+', '')\n",
        "\n",
        "# # The maximum number of words to be used. (most frequent)\n",
        "# MAX_NB_WORDS = 50000\n",
        "# # Max number of words in each complaint.\n",
        "# MAX_SEQUENCE_LENGTH = 100\n",
        "# # This is fixed.\n",
        "# EMBEDDING_DIM = 100\n",
        "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "# tokenizer.fit_on_texts(df['Tweet'].values)\n",
        "# word_index = tokenizer.word_index\n",
        "# print('Found %s unique tokens.' % len(word_index))\n",
        "# tokenizer2 = tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS9LDoRpTXY_"
      },
      "source": [
        "# Find most retweeted tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM8UQ2KnKtne"
      },
      "source": [
        "# Code to find top 1-1500 retweeted people\n",
        "# Kind of binary search between min and max of tweets present\n",
        "def FindMostRetweeted(hashtag,max_taken = 4300000,min_taken=0):\n",
        "  \n",
        "  if(max_taken <= min_taken+2):\n",
        "    print('revert to previous answer')\n",
        "    return None\n",
        "  p_val = \"\"\n",
        "  val = int((max_taken+min_taken)/2)\n",
        "  c = twint.Config()\n",
        "  c.Limit = 2000\n",
        "  c.Search = hashtag\n",
        "  c.Min_retweets = val\n",
        "  c.Store_csv = True\n",
        "  csv_name = \"temp_out\"+str(val)+\".csv\"\n",
        "  c.Output = csv_name\n",
        "  twint.run.Search(c)\n",
        "  \n",
        "  t_df = None\n",
        "  if(path.exists(csv_name)):\n",
        "    p_val += '\\ntweets found for '+str(val)\n",
        "    t_df = pd.read_csv(csv_name, delimiter=',',names = col_names,skiprows=1)\n",
        "    IPython.display.clear_output()\n",
        "  \n",
        "  t1_df = None\n",
        "  if((type(t_df)==type(None)) or (t_df.id.count() < 1)):\n",
        "    # no tweets found \n",
        "    p_val += '\\nneed more people : less rt '+str(val)\n",
        "    t1_df = FindMostRetweeted(hashtag,val,min_taken)\n",
        "  elif(t_df.id.count() > 1500):\n",
        "    # too many found\n",
        "    val1 = sorted(list(t_df.retweets_count))[-15]\n",
        "    p_val+= '\\nneed less people : more rt '+str(val1)\n",
        "    t1_df = FindMostRetweeted(hashtag,max_taken,val1)\n",
        "  else:\n",
        "    # correct number found\n",
        "    p_val+= '\\nFound at '+str(val)+', tweets = '+str(t_df.id.count())\n",
        "  \n",
        "  try:\n",
        "    os.remove(csv_name)\n",
        "  except:\n",
        "    p_val += \"\\nno tweets found for \"+str(val)\n",
        "\n",
        "  if(not (type(t1_df) == type(None))):\n",
        "    print(p_val)\n",
        "    return t1_df\n",
        "  print(p_val)\n",
        "  return t_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egWX1cKOSYPd"
      },
      "source": [
        "# most retweeted tweets found\n",
        "df_most = FindMostRetweeted('caa OR nrc OR npr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ZN8uqgTfdn"
      },
      "source": [
        "# Get all Retweets, Original tweets and Replies for a user by user_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrWaCfxei3j7"
      },
      "source": [
        "# Use twint search to get the retweets by user which have that hashtag\n",
        "def getAllRetweets(usr_id,dir_path,hashtag=''):\n",
        "  c = twint.Config()\n",
        "  c.User_id = usr_id\n",
        "  c.Replies = False\n",
        "  if(hashtag!=''):\n",
        "    c.Search= hashtag\n",
        "  c.Retweets = True\n",
        "  c.Native_retweets = True\n",
        "  c.Store_csv = True\n",
        "  csv_name = dir_path+\"rts.csv\"\n",
        "  c.Output = csv_name\n",
        "  twint.run.Search(c)\n",
        "\n",
        "  df = None\n",
        "  if(path.exists(csv_name)):\n",
        "    df = pd.read_csv(csv_name, delimiter=',',names = col_names,skiprows=1)\n",
        "    IPython.display.clear_output()\n",
        "  return df\n",
        "\n",
        "# Use twint search to get the original tweets and replies by user which have that hashtag\n",
        "def getAllOriginalAndReplies(usr_id,dir_path,hashtag=''):\n",
        "  c = twint.Config()\n",
        "  c.User_id = usr_id\n",
        "  if(hashtag!=''):\n",
        "    c.Search= hashtag\n",
        "  c.Replies = False\n",
        "  c.Retweets = False\n",
        "  c.Native_retweets = False\n",
        "  c.Store_csv = True\n",
        "  csv_name = dir_path+\"og_rep.csv\"\n",
        "  c.Output = csv_name\n",
        "  twint.run.Search(c)\n",
        "\n",
        "  df1 = None\n",
        "  df2 = None\n",
        "  if(path.exists(csv_name)):\n",
        "    df = pd.read_csv(csv_name, delimiter=',',names = col_names,skiprows=1)\n",
        "    IPython.display.clear_output()\n",
        "    df1 = df[df['id'] == df['conversation_id']] #Original Tweets\n",
        "    df1 = df1.reset_index(drop=True)\n",
        "    df2 = df[df['id'] != df['conversation_id']] #Reply Tweets\n",
        "    df2 = df2.reset_index(drop=True)\n",
        "  return df1,df2\n",
        "\n",
        "# Calls all the above functions to send the set of dataf\n",
        "def getTweets(usr_id,hashtag=''):\n",
        "  dir_path = '/content/drive/My Drive/TweetData/tweets_'+str(usr_id)\n",
        "  # cached\n",
        "  if(usr_id in all_dfs.keys()):\n",
        "    # dfs = all_dfs[usr_id]['dataframes']\n",
        "    # return dfs[0],dfs[1],dfs[2],dir_path\n",
        "    return None,None,None,\"already there\"\n",
        "  try:\n",
        "    os.mkdir(dir_path)\n",
        "    df1 = getAllRetweets(usr_id,dir_path+'/',hashtag)\n",
        "    df2,df3 = getAllOriginalAndReplies(usr_id,dir_path+'/',hashtag)\n",
        "    try:\n",
        "      os.rmdir(dir_path)\n",
        "    except:\n",
        "      # print('tweets present')\n",
        "      ;\n",
        "    return df1,df2,df3,dir_path\n",
        "  except:\n",
        "    # print(\"already there\")\n",
        "    return None,None,None,\"already there\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wocFpu6aUUhw"
      },
      "source": [
        "# Get the stance predicted by model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAfgDf0_CLhp"
      },
      "source": [
        "RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
        "tag_pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
        "\n",
        "def hashtag_to_sent(tag):\n",
        "    return \" \".join(tag_pattern.findall(tag))\n",
        "\n",
        "def convert_hashtags(twt):\n",
        "  word_list = twt.split(' ')\n",
        "  twt = \"\"\n",
        "  for word in word_list:\n",
        "    if(word.startswith('#')):\n",
        "      twt+=' '+hashtag_to_sent(word[1:])\n",
        "    else:\n",
        "      twt+=' '+word\n",
        "  return twt\n",
        "\n",
        "def translate_tweet(twts):\n",
        "  tw_list = [convert_hashtags(RE_EMOJI.sub(r'', twt)) for twt in twts]\n",
        "  try:\n",
        "    # trans_twt = translator.translate(convert_hashtags(twt),dest='en').text\n",
        "    translations = translator.translate(tw_list, dest='en')\n",
        "    return[t.text for t in translations]\n",
        "  except:\n",
        "    trans_twt = [twt.encode('utf-8').decode() for twt in twts] \n",
        "    # print(trans_twt)\n",
        "    print('translator failed')\n",
        "    return trans_twt\n",
        "\n",
        "def getPredictions(tweet_list):\n",
        "  tweet_list = translate_tweet(tweet_list)\n",
        "  Ltest = glove_test(glove_word_vec_dict,tweet_list)\n",
        "  results = []\n",
        "  fin = {'F':0,'A':0,'N':0}\n",
        "  labels_stance = {0:\"A\", 1:\"F\", 2:\"N\"}\n",
        "\n",
        "  for classifier in filenames[2:5]:\n",
        "    res_arr = models1[classifier].predict(Ltest)\n",
        "    temp = {'F':0,'A':0,'N':0,\"\":0}\n",
        "    for i in res_arr:\n",
        "      temp[labels_stance[i%3]]+=1\n",
        "      fin[labels_stance[i%3]]+=1\n",
        "    tot = sum(temp.values())\n",
        "    temp = {key:temp[key]/tot for key in temp.keys()}\n",
        "    if(temp['A'] > temp['F']):\n",
        "      temp['res'] = 'A'\n",
        "    elif(temp['A'] < temp['F']):\n",
        "      temp['res'] = 'F'\n",
        "    else:\n",
        "      temp['res'] = 'N'\n",
        "    results.append(temp['res'])\n",
        "  \n",
        "  # for classifier in filenames[8:16]:\n",
        "  #   res_arr = models2[classifier].predict(Ltest)\n",
        "  #   temp = {'F':0,'A':0,'N':0}\n",
        "  #   for i in res_arr:\n",
        "  #     temp[labels_stance[i%3]]+=1\n",
        "  #     # fin[labels_stance[i]]+=1\n",
        "  #   tot = sum(temp.values())\n",
        "  #   temp = {key:temp[key]/tot for key in temp.keys()}\n",
        "  #   if(temp['A'] > temp['F']):\n",
        "  #     temp['res'] = 'A'\n",
        "  #   elif(temp['A'] < temp['F']):\n",
        "  #     temp['res'] = 'F'\n",
        "  #   else:\n",
        "  #     temp['res'] = 'N'\n",
        "  #   results.append(temp)\n",
        "  \n",
        "  tst = tokenizer1.texts_to_sequences(tweet_list)\n",
        "  tst = pad_sequences(tst, maxlen=100)\n",
        "  for pkl_filename in [filenames[17]]:\n",
        "    res_arr = models3[pkl_filename].predict(tst)\n",
        "    temp = {'F':0,'A':0,'N':0}\n",
        "    for i in res_arr:\n",
        "      temp[labels_stance[0]]+=i[0]\n",
        "      temp[labels_stance[1]]+=i[1]\n",
        "      temp[labels_stance[2]]+=i[2]\n",
        "    # temp = {key:int(round(temp[key])) for key in temp.keys()}\n",
        "    # fin = {key: (fin[key]+temp[key]) for key in temp.keys()}\n",
        "    tot = sum(temp.values())\n",
        "    temp = {key:temp[key]/tot for key in temp.keys()}\n",
        "    if(temp['A'] > temp['F']):\n",
        "      temp['res'] = 'A'\n",
        "    elif(temp['A'] < temp['F']):\n",
        "      temp['res'] = 'F'\n",
        "    else:\n",
        "      temp['res'] = 'N'\n",
        "    results.append(temp['res'])\n",
        "\n",
        "  # tst1 = tokenizer2.texts_to_sequences(tweet_list)\n",
        "  # tst1 = pad_sequences(tst1, maxlen=100)\n",
        "  # for pkl_filename in filenames[18:]:\n",
        "  #   res_arr = models4[pkl_filename].predict(tst1)\n",
        "  #   temp = {'F':0,'A':0,'N':0}\n",
        "  #   for i in res_arr:\n",
        "  #     temp[labels_stance[0]]+=i[0]\n",
        "  #     temp[labels_stance[1]]+=i[1]\n",
        "  #     temp[labels_stance[2]]+=i[2]\n",
        "  #   # temp = {key:int(round(temp[key])) for key in temp.keys()}\n",
        "  #   # fin = {key:fin[key]+temp[key] for key in temp.keys()}\n",
        "  #   tot = sum(temp.values())\n",
        "  #   temp = {key:temp[key]/tot for key in temp.keys()}\n",
        "  #   if(temp['A'] > temp['F']):\n",
        "  #     temp['res'] = 'A'\n",
        "  #   elif(temp['A'] < temp['F']):\n",
        "  #     temp['res'] = 'F'\n",
        "  #   else:\n",
        "  #     temp['res'] = 'N'\n",
        "  #   results.append(temp)\n",
        "\n",
        "  # df_res = pd.DataFrame(results)\n",
        "  # display(results)\n",
        "  \n",
        "  # if( (results[3]=='F'and (results[0] == 'F' or results[2] == 'F')) or (results[2] == 'F')):\n",
        "  if( results.count('F') >= 2 ):\n",
        "    return 'F'\n",
        "  elif( results.count('A') > 2 ):\n",
        "    return 'A'\n",
        "  else:\n",
        "    return 'N'\n",
        "  # return df_res['res'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56IXapHsUeWK"
      },
      "source": [
        "# Get the propaganda and bias metric of user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z00zIbyb4XxA"
      },
      "source": [
        "### Characteristics of users who spread propaganda: \n",
        "\n",
        "(1) **sending high volumes of tweets over short periods of time** - check if time graph has peaks \n",
        "\n",
        "(2) **retweeting whilepublishing little original content** \n",
        "\n",
        "(3) **quickly retweeting**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2DdlSngexaN"
      },
      "source": [
        "### *(1) high tweet volume*\n",
        "\n",
        "Avg person reads 200 words a minute with 60% comprehension\n",
        "\n",
        "300-500 ms for regression + comprehension\n",
        "\n",
        "Avg political tweet length + hindi tweet length 75\n",
        "(280 characters allowed per tweet)\n",
        "\n",
        "Avg char per word 5.1\n",
        "\n",
        "Hence, Avg tweet words = 75/5 = 15\n",
        "\n",
        "time per tweet = 15*(200/60) + 0.5 s = 50.5s\n",
        "\n",
        "hence understandable tweets/minute = 1.18\n",
        "\n",
        "Hence, if a person retweets 2 or more tweets per minute, they are participating in ill-informed propaganda\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "METRIC : N1/( total )\n",
        "\n",
        "where N1 = tweets that are above 2 per minute in all categories\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgFRGadSjg3z"
      },
      "source": [
        "### *(2) less original content*\n",
        "\n",
        "If retweet percentage much higher than both original and replies then - ill-informed propaganda (contributes more bias)\n",
        "\n",
        "If retweet percentage very high but replies high too - informed propaganda (contributed less bias)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "MERTIC : Retweet% - Original% - (Reply%)/2\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur4kW_BVlQVX"
      },
      "source": [
        "### *(3) quick retweeting*\n",
        "\n",
        "If time difference between original tweet time and retweet time is less than 50s it was mindlessly forwarded\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "MERTIC : N2/( total retweets )\n",
        "\n",
        "where N2 = number of rts retweeted quickly\n",
        "\n",
        "---\n",
        " combine with metric 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6ZFyeEUX7Gm"
      },
      "source": [
        "datetimeFormat = '%Y-%m-%d %H:%M:%S'\n",
        "def getMetrics(df1,df2,df3,vals):\n",
        "  # M1 = unique list of:\n",
        "  # 1. anything in combo that was closer than 50 seconds\n",
        "  # 2. any in retweets that are closer to original than 50 seconds\n",
        "  # make unique list of these propaganda tweets\n",
        "  # M2 = %(retweets) - %(originals) - (%replies)/2\n",
        "  df_list = []\n",
        "  prop_tweet = set()\n",
        "\n",
        "  # for m1 - 2\n",
        "  if(type(df1) != type(None)):\n",
        "    for ind in df1.index: \n",
        "        date1 = str(df1['date'][ind])+' '+str(df1['time'][ind]) \n",
        "        date2 = str(df1['retweet_date'][ind])\n",
        "        diff = datetime.datetime.strptime(date2, datetimeFormat) - datetime.datetime.strptime(date1, datetimeFormat)\n",
        "        if(diff.total_seconds() < 51):\n",
        "          prop_tweet.add(df1['id'][ind])\n",
        "    df_list.append(df1)\n",
        "\n",
        "  if(type(df2) != type(None)):\n",
        "    df_list.append(df2)\n",
        "\n",
        "  if(type(df3) != type(None)):\n",
        "    df_list.append(df3)\n",
        "\n",
        "  if(df_list == []):\n",
        "    return 0,0\n",
        "\n",
        "  # for m1 - 1 and stance\n",
        "  df_comb = pd.concat(df_list)\n",
        "  df_comb = df_comb[['date','time','id']]\n",
        "  df_comb.reset_index(inplace=True, drop=True) \n",
        "  df_comb[\"datetime\"] = df_comb['date'].astype(str)+' '+df_comb['time'].astype(str)\n",
        "  final_df = df_comb.sort_values(by=['datetime'])\n",
        "\n",
        "  # tweet_times = []\n",
        "  for ind in range(len(final_df['datetime']) - 1):\n",
        "    date1 = final_df[\"datetime\"][final_df.index[ind]]\n",
        "    date2 = final_df[\"datetime\"][final_df.index[ind+1]]\n",
        "    diff = datetime.datetime.strptime(date2, datetimeFormat) - datetime.datetime.strptime(date1, datetimeFormat)\n",
        "    # tweet_times.append(diff.total_seconds())\n",
        "    if(diff.total_seconds() < 51):\n",
        "      prop_tweet.add(final_df[\"id\"][ind])\n",
        "    \n",
        "  # percentage of ill-understood tweets\n",
        "  m1 = len(prop_tweet)*100/len(df_comb['id'])\n",
        "  # plt.plot(tweet_times)\n",
        "  # plt.title(str(m1)+'% tweets propaganda - '+id)\n",
        "  # plt.xlabel('difference in seconds')\n",
        "  # plt.ylabel('number of tweets')\n",
        "  # plt.savefig('/content/drive/My Drive/Graphs/graph7.png')\n",
        "  # plt.show()\n",
        "  m2 = (vals[0]/vals[3] - vals[1]/vals[3] - vals[2]/(vals[3]*2))*100\n",
        "  # print(m2)\n",
        "  if(m2 < 0):\n",
        "    m2 = 0\n",
        "  return m1,m2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBVhEGhgTvCJ"
      },
      "source": [
        "# Load a user's details from drive,twint or cache"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJAKoKGY7aV8"
      },
      "source": [
        "all_dfs = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4Zz6ixcEuvZ",
        "outputId": "1e1c595e-a395-4eac-81a6-84d0e4042cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#general get function\n",
        "all_pths = [f for f in listdir('/content/drive/My Drive/TweetData')]\n",
        "print(len(all_pths))\n",
        "def getUser(user_id,offline=False):\n",
        "  \n",
        "  if(user_id in all_dfs.keys()):\n",
        "    # print('present in array')\n",
        "    return True\n",
        "  \n",
        "  if(not offline):\n",
        "    df1, df2, df3, pth = getTweets(user_id,'caa OR nrc OR npr')\n",
        "    \n",
        "  if(offline or pth == \"already there\" ):\n",
        "    pth = '/content/drive/My Drive/TweetData/tweets_'+str(user_id)\n",
        "    try:\n",
        "      df1 = pd.read_csv(pth+\"/rts.csv\", delimiter=',',names = col_names,skiprows=1)\n",
        "    except:\n",
        "      df1 = None\n",
        "    try:\n",
        "      df = pd.read_csv(pth+\"/og_rep.csv\", delimiter=',',names = col_names,skiprows=1)\n",
        "      df2 = df[df['id'] == df['conversation_id']] #Original Tweets\n",
        "      df2 = df2.reset_index(drop=True)\n",
        "      df3 = df[df['id'] != df['conversation_id']] #Reply Tweets\n",
        "      df3 = df3.reset_index(drop=True)\n",
        "      stance = getPredictions(df['tweet'].values)\n",
        "    except:\n",
        "      # print(traceback.format_exc())\n",
        "      stance = 'N'\n",
        "      df2 = None\n",
        "      df3 = None\n",
        "    # print('loaded from drive')\n",
        "  else:\n",
        "    all_pths = [f for f in listdir('/content/drive/My Drive/TweetData')]\n",
        "    # print('downloaded from twint')\n",
        "  \n",
        "  dfs = [df1, df2, df3]\n",
        "  if(type(df1) !=  type(None)):\n",
        "    c1 = df1.id.count()\n",
        "  else:\n",
        "    c1 = 0\n",
        "  if(type(df2) !=  type(None)):\n",
        "    c2 = df2.id.count()\n",
        "  else:\n",
        "    c2 = 0\n",
        "  if(type(df3) !=  type(None)):\n",
        "    c3 = df3.id.count()\n",
        "  else:\n",
        "    c3 = 0\n",
        "  t = c1+c2+c3\n",
        "\n",
        "  if(t!=0):\n",
        "    m1,m2 = getMetrics(dfs[0],dfs[1],dfs[2],[c1,c2,c3,t])\n",
        "    all_dfs[user_id] = {'dataframes':dfs,'values':[c1,c2,c3,t]}\n",
        "    all_dfs[user_id]['metrics'] = [m1,m2]\n",
        "    all_dfs[user_id]['stance'] = stance\n",
        "    return True\n",
        "  else:\n",
        "    # print(user_id_i,\" error\")\n",
        "    return False\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6847\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYkdqZWVIWGh"
      },
      "source": [
        "## Populate the ego of most retweeted tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewWJ7g1USboi"
      },
      "source": [
        "influencial_users = list(set(df_most['user_id']))\n",
        "for user_id_i in influencial_users:\n",
        "  getUser(user_id_i)\n",
        "  lst_rts = []\n",
        "  for tw_id in all_dfs[user_id_i]['dataframes'][1]['id']:\n",
        "    rts = api.retweets(str(tw_id),101)\n",
        "    for rt in rts:\n",
        "      lst_rts.append(rt._json['user']['id'])\n",
        "\n",
        "  for user_id in lst_rts:\n",
        "    getUser(user_id)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJmAKQEAo-PU"
      },
      "source": [
        "## Expansion of tweet graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxuzhxFdo-Pe"
      },
      "source": [
        "lst_old = list(all_dfs.keys())\n",
        "inferred_stance = {}\n",
        "for i in lst_old:\n",
        "  df1,df2,df3 = all_dfs[i]['dataframes']\n",
        "  # add writers of retweets\n",
        "  rt_stances = []\n",
        "  if(type(df1) != type(None)):\n",
        "    for new_id in list(set(df1['user_id'])):\n",
        "      getUser(new_id)\n",
        "        \n",
        "  # add people replied to \n",
        "  if(type(df3) != type(None)):\n",
        "    for ind in df3.index:\n",
        "      try:\n",
        "        new_id =  ast.literal_eval(df3['reply_to'][ind])[1]['user_id']\n",
        "        getUser(new_id)\n",
        "      except:\n",
        "        ;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11E32LncIqUh"
      },
      "source": [
        "### Load all users whose data is present in drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l9jadvn3zEv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "531e46a1-fbb5-408c-ebcc-24d2c245d469"
      },
      "source": [
        "# load all from memory\n",
        "all_pths = [f for f in listdir('/content/drive/My Drive/TweetData')]\n",
        "for pth1 in all_pths: \n",
        "  user_id_i = pth1.split(\"_\",1)[1]\n",
        "  print(getUser(user_id_i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "translator failed\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXHjRagoIc-J"
      },
      "source": [
        "# Get inferred stances\n",
        "\n",
        "Higher reliability on stance predicted for users with lot of original content. Hence, more confidence in tweets that are inferred then on model predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwDfn_XWqBUM"
      },
      "source": [
        "# Check stances with retweets\n",
        "lst_old = list(all_dfs.keys())\n",
        "inferred_stance = {}\n",
        "for i in lst_old:\n",
        "  df1,df2,df3 = all_dfs[i]['dataframes']\n",
        "  # add writers of retweets\n",
        "  rt_stances = []\n",
        "  if(type(df1) != type(None)):\n",
        "    for new_id in list(set(df1['user_id'])):\n",
        "      if(getUser(new_id,True)):\n",
        "        rt_stances.append(all_dfs[new_id]['stance'])\n",
        "  if(rt_stances == []):\n",
        "    continue\n",
        "  if(rt_stances.count('F') > rt_stances.count('A')):\n",
        "    inferred_stance[i] = 'F'\n",
        "  elif(rt_stances.count('F') < rt_stances.count('A')):\n",
        "    inferred_stance[i] = 'A'\n",
        "  else:\n",
        "    inferred_stance[i] = 'N'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giI2erKnGLZg",
        "outputId": "8fbece24-783b-4739-d045-14514af5ebb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "wr = []\n",
        "for i in inferred_stance:\n",
        "  if(inferred_stance[i] != all_dfs[i]['stance'] ):\n",
        "    wr.append(i)\n",
        "print(len(wr))\n",
        "print(len(inferred_stance))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2195\n",
            "3070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NafVZf7yV1Fz"
      },
      "source": [
        "p_list = []\n",
        "for i in all_dfs:\n",
        "  temp = {}\n",
        "  temp['id'] = i\n",
        "  temp['ret'] = all_dfs[i]['values'][0]\n",
        "  temp['og'] = all_dfs[i]['values'][1]\n",
        "  temp['rep'] = all_dfs[i]['values'][2]\n",
        "  temp['tot'] = all_dfs[i]['values'][3]\n",
        "  temp['m1'] = all_dfs[i]['metrics'][0]\n",
        "  temp['m2'] = all_dfs[i]['metrics'][1]\n",
        "  if(i in inferred_stance.keys()):\n",
        "    temp['stance'] = inferred_stance[i]\n",
        "  else:\n",
        "    temp['stance'] = all_dfs[i]['stance']\n",
        "  p_list.append(temp)\n",
        "\n",
        "df_metrics1 = pd.DataFrame(p_list) \n",
        "df_metrics1.to_csv('/content/drive/My Drive/metrics_inferred1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpKVnp6SVNMu"
      },
      "source": [
        "### Testing stance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYqr2LkyVPzd"
      },
      "source": [
        "interest_u = ['85657578','1153045459','17537467','18839785','889481829966782466','3171712086','1215480693200805888']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_htbwZ28tRm",
        "outputId": "bbda786e-f795-44a5-f566-92725624f545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "for u_id in list(set(interest_u)):\n",
        "  if(getUser(u_id)):\n",
        "    s_res = all_dfs[u_id]['stance']\n",
        "    display(set(all_dfs[u_id]['dataframes'][1]['username']))\n",
        "    print(s_res)\n",
        "  else:\n",
        "    print(u_id,' error')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'incindia'}"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'tarekfatah'}"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "F\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'shrutiv_vyas'}"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "N\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'shaheenbag'}"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'mrsgandhi'}"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "F\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'rahulgandhi'}"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'narendramodi'}"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GDwaeE-vN0v"
      },
      "source": [
        "### Present the metrics according to stance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_A8NKupAqMe"
      },
      "source": [
        "def present_metrics(df_metrics,topic):\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "  print(topic)\n",
        "  \n",
        "  print('\\nFOR')\n",
        "  tf = len(df_metrics[df_metrics['stance'] == 'F'])\n",
        "  print('total users',tf)\n",
        "  pf = len(df_metrics[df_metrics['stance'] == 'F'][df_metrics['m1'] > 0])\n",
        "  print('total propaganda users',pf)\n",
        "  print('propaganda user % ',pf*100/tf)\n",
        "  print('mean propaganda',df_metrics[df_metrics['stance'] == 'F']['m1'].mean())\n",
        "  print('total unoriginal users',len(df_metrics[df_metrics['stance'] == 'F'][df_metrics['m2'] > 0]))\n",
        "  print('mean unoriginality',df_metrics[df_metrics['stance'] == 'F']['m2'].mean())\n",
        "\n",
        "  print('\\nAGAINST')\n",
        "  tf = len(df_metrics[df_metrics['stance'] == 'A'])\n",
        "  print('total users',tf)\n",
        "  pf = len(df_metrics[df_metrics['stance'] == 'A'][df_metrics['m1'] > 0])\n",
        "  print('total propaganda users',pf)\n",
        "  print('propaganda user % ',pf*100/tf)\n",
        "  print('mean propaganda',df_metrics[df_metrics['stance'] == 'A']['m1'].mean())\n",
        "  print('total unoriginal users',len(df_metrics[df_metrics['stance'] == 'A'][df_metrics['m2'] > 0]))\n",
        "  print('mean unoriginality',df_metrics[df_metrics['stance'] == 'A']['m2'].mean())\n",
        "\n",
        "  print('\\nNONE')\n",
        "  tf = len(df_metrics[df_metrics['stance'] == 'N'])\n",
        "  print('total users',tf)\n",
        "  pf = len(df_metrics[df_metrics['stance'] == 'N'][df_metrics['m1'] > 0])\n",
        "  print('total propaganda users',pf)\n",
        "  print('propaganda user % ',pf*100/tf)\n",
        "  print('mean propaganda',df_metrics[df_metrics['stance'] == 'N']['m1'].mean())\n",
        "  print('total unoriginal users',len(df_metrics[df_metrics['stance'] == 'N'][df_metrics['m2'] > 0]))\n",
        "  print('mean unoriginality',df_metrics[df_metrics['stance'] == 'N']['m2'].mean())\n",
        "  display(HTML(\"<hr/>\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o2Bj4CfKvMo",
        "outputId": "788ab493-f267-4411-e6cc-15daec447052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "import warnings\n",
        "# present_metrics(df_metrics,'Using model stance')\n",
        "present_metrics(df_metrics1,'Using stance from model + inferring from graph')\n",
        "df_metrics2 = df_metrics1[df_metrics1['tot'] > 15]\n",
        "present_metrics(df_metrics2,'Filtered users')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using stance from model + inferring from graph\n",
            "\n",
            "FOR\n",
            "total users 2051\n",
            "total propaganda users 578\n",
            "propaganda user %  28.18137493905412\n",
            "mean propaganda 2.6192086247181017\n",
            "total unoriginal users 505\n",
            "mean unoriginality 17.36165829563697\n",
            "\n",
            "AGAINST\n",
            "total users 2642\n",
            "total propaganda users 591\n",
            "propaganda user %  22.369417108251326\n",
            "mean propaganda 1.9171069356631656\n",
            "total unoriginal users 867\n",
            "mean unoriginality 23.573975872582228\n",
            "\n",
            "NONE\n",
            "total users 2717\n",
            "total propaganda users 338\n",
            "propaganda user %  12.440191387559809\n",
            "mean propaganda 1.8220227919118392\n",
            "total unoriginal users 492\n",
            "mean unoriginality 14.263956791415103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<hr/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Filtered users\n",
            "\n",
            "FOR\n",
            "total users 956\n",
            "total propaganda users 487\n",
            "propaganda user %  50.94142259414226\n",
            "mean propaganda 3.793881034780552\n",
            "total unoriginal users 73\n",
            "mean unoriginality 3.5722842162070836\n",
            "\n",
            "AGAINST\n",
            "total users 1173\n",
            "total propaganda users 516\n",
            "propaganda user %  43.989769820971865\n",
            "mean propaganda 3.1930287846959207\n",
            "total unoriginal users 133\n",
            "mean unoriginality 6.097662311786741\n",
            "\n",
            "NONE\n",
            "total users 430\n",
            "total propaganda users 196\n",
            "propaganda user %  45.58139534883721\n",
            "mean propaganda 4.116339669103711\n",
            "total unoriginal users 21\n",
            "mean unoriginality 1.8950689377532417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<hr/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRrzbzm3L7pr"
      },
      "source": [
        "Hence, even though the number of people with stance of 'FOR' and 'AGAINST' are different, the higher propaganda on the 'FOR' side"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM5RpgCjUthl"
      },
      "source": [
        "### Observation at 4000 users "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av8nQP6oBH1w",
        "outputId": "76d6591d-ddaf-479c-fa0b-fac00ff3e98d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df_metrics['m1'].mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9446539643335663"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKZDYWJXaH0s",
        "outputId": "d7c9fba8-bbc6-424f-8160-332a1d2b222b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('percentage of all current users that participate in propaganda ')\n",
        "len(df_metrics[df_metrics['m1']>0]['id'])*100/len(df_metrics['id'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " percentage of all current users that participate in propaganda \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.10922604621102"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHcSCAF5sb7B"
      },
      "source": [
        "### Checking for bots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw0bC0ZYseK4"
      },
      "source": [
        "#x-rapidapi-key\": \"930c3333aamsh6abe1fa37394f1dp1d3792jsne8c0dd43343f\n",
        "import botometer\n",
        "rapidapi_key = \"930c3333aamsh6abe1fa37394f1dp1d3792jsne8c0dd43343f\" # now it's called rapidapi key\n",
        "twitter_app_auth = {\n",
        "    'consumer_key': 'aki0CkgST6uEPvQQAbApbgSjU',\n",
        "    'consumer_secret': 'GV63N7Gow7jM8MS7n1P7AbBg244kpjwp7jlfH8hoiWCkvgQ6CT',\n",
        "    'access_token': '2199543638-X9myvLXlyDt7AtUkB7TEcSOYYdqPAtPMjcAFJ5J',\n",
        "    'access_token_secret': '7B3LTEQUwDXh3zrDvOkBdi0pHXAxDzHInE64yDXoDw6yV',\n",
        "  }\n",
        "bom = botometer.Botometer(wait_on_ratelimit=True,\n",
        "                          rapidapi_key=rapidapi_key,\n",
        "                          **twitter_app_auth)\n",
        "\n",
        "def getBotScore(usr_id):\n",
        "  result = bom.check_account(int(usr_id))\n",
        "  return max(result['cap']['english'],result['cap']['universal'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT15Xds52Xn6",
        "outputId": "65ccc7a0-081b-4598-8ddd-161a1e21c026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "list1 = list(df_metrics1.id.values)\n",
        "ind = 0\n",
        "while(ind < len(list1)):\n",
        "  bot_res = []\n",
        "  for i in list1[ind:]:\n",
        "        try:\n",
        "          val = getBotScore(i)\n",
        "          bot_res.append({\"id\":i,\"cap\":val})\n",
        "        except:\n",
        "          print(sys.exc_info()[0])\n",
        "          print('error at '+str(i))\n",
        "          last_val = i\n",
        "          break\n",
        "  df_bot = pd.DataFrame(bot_res) \n",
        "  df_bot.to_csv('/content/drive/My Drive/bot_data/bot_file'+str(ind)+'.csv')\n",
        "  ind = list1.index(last_val)+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tweepy.error.TweepError'>\n",
            "error at 942748218424623105\n",
            "<class 'tweepy.error.TweepError'>\n",
            "error at 714265230721699840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqtufrYFO1WA"
      },
      "source": [
        "all_pthsb = [f for f in listdir('/content/drive/My Drive/bot_data')]\n",
        "dfsb = []\n",
        "for fb in all_pthsb:\n",
        "  dfb = pd.read_csv('/content/drive/My Drive/bot_data/'+fb, delimiter=',',names = ['id','cap'],skiprows=1)\n",
        "  dfsb.append(dfb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta7T2oGcT3dE"
      },
      "source": [
        "df_caps = pd.concat(dfsb,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2DFTywM9DW1",
        "outputId": "2015ca8e-b3ba-4e50-cb23-820e75d914aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "statistics.mean(df_caps['cap'].values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05264535721879503"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-wR3NpKUfDq",
        "outputId": "ab9cd59d-8bb1-40c8-a787-ac7fa6c52c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "stncs = []\n",
        "for i in df_caps['id'].values:\n",
        "  stncs.append(df_metrics1[df_metrics1['id'] == i]['stance'].values[0])\n",
        "len(stncs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "285"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5ZJCzzQVSoM"
      },
      "source": [
        "df_caps['stance'] = stncs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eewtqbOBVYUx",
        "outputId": "b28e077a-e201-4fe2-e06a-860c58aee3aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print('A',statistics.mean(df_caps[df_caps['stance'] == 'A']['cap'].values))\n",
        "print('F',statistics.mean(df_caps[df_caps['stance'] == 'F']['cap'].values))\n",
        "print('N',statistics.mean(df_caps[df_caps['stance'] == 'N']['cap'].values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A 0.04660138435267751\n",
            "F 0.05989869426898036\n",
            "N 0.05255728219975074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af3ugj9VVu52",
        "outputId": "9e87e93c-cb42-4f03-8abe-112c4e405689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print('A',df_caps[df_caps['stance'] == 'A']['id'].count())\n",
        "print('F',df_caps[df_caps['stance'] == 'F']['id'].count())\n",
        "print('N',df_caps[df_caps['stance'] == 'N']['id'].count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A 87\n",
            "F 74\n",
            "N 124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFuvuJHgWDhk",
        "outputId": "3190f409-95d0-4540-b7f8-d203b14129bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "print(df_caps[df_caps['cap'] > 0.50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      id       cap stance\n",
            "9    1200420801876500481  0.641146      N\n",
            "69   1102905529980968961  0.938706      N\n",
            "226  1145902517462564870  0.511452      F\n",
            "231   794792628638322689  0.511452      N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRMNUcdBp-xI"
      },
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAUBbqsPrbNb"
      },
      "source": [
        "df_metrics1 = pd.read_csv('/content/drive/My Drive/metrics_inferred1.csv', delimiter=',',names = ['id','ret','og','rep','tot','m1','m2','stance'],skiprows=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agQh9L8NsSDd"
      },
      "source": [
        "def getColor(id):\n",
        "  if(id in df_metrics1['id'].values):\n",
        "    if(df_metrics1[df_metrics1['id'] == id]['stance'].values == ['F']):\n",
        "        return 'green'\n",
        "    elif(df_metrics1[df_metrics1['id'] == id]['stance'].values == ['A']):\n",
        "        return 'red' \n",
        "    else:\n",
        "      return 'white'\n",
        "  else:\n",
        "    if(all_dfs[id]['stance'] == 'F'):\n",
        "        return 'green'\n",
        "    elif(all_dfs[id]['stance'] == ['A']):\n",
        "        return 'red' \n",
        "    else:\n",
        "      return 'white'\n",
        "    \n",
        "# pos=nx.spring_layout(G)\n",
        "# plt.figure(1,figsize=(12,12)) \n",
        "# nx.draw(G,node_color=color_map,node_size=2)\n",
        "# plt.savefig('/content/drive/My Drive/Graphs/graph10.png')\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc6xpPeXb6k3"
      },
      "source": [
        "G = nx.DiGraph()\n",
        "nf = 0\n",
        "for i in all_dfs:\n",
        "  df1,df2,df3 = all_dfs[i]['dataframes']\n",
        "  if(type(df1) != type(None)):\n",
        "    if(i not in G.nodes):\n",
        "        G.add_node(i,color=getColor(i))\n",
        "    for new_id in list(set(df1['user_id'])):\n",
        "      if(new_id in all_dfs.keys()):\n",
        "        if(new_id not in G.nodes):\n",
        "          G.add_node(new_id,color=getColor(new_id))\n",
        "        G.add_edge(new_id, i, weight=2)\n",
        "      else:\n",
        "        nf+=1\n",
        "\n",
        "  if(type(df3) != type(None)):\n",
        "    if(i not in G.nodes):\n",
        "        G.add_node(i,color=getColor(i))\n",
        "    for ind in df3.index:\n",
        "      try:\n",
        "        new_id =  ast.literal_eval(df3['reply_to'][ind])[1]['user_id']\n",
        "        if( new_id in all_dfs.keys()):\n",
        "          if(new_id not in G.nodes):\n",
        "            G.add_node(new_id,color=getColor(new_id))\n",
        "          G.add_edge(i, new_id, weight=1)\n",
        "        else:\n",
        "          nf+=1\n",
        "      except:\n",
        "        # reply to self\n",
        "        continue\n",
        "\n",
        "nx.write_gexf(G, \"test.gexf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDaMtKs7atH5"
      },
      "source": [
        "def plot_activity(df1,title,kind='line'):\n",
        "  df1[\"date\"] = df1[\"date\"].astype(\"datetime64\")\n",
        "  df1[\"time\"] = df1[\"time\"].astype(\"datetime64\")\n",
        "  df1 = df1[[\"time\",\"date\",\"tweet\"]]\n",
        "  return df1.groupby([df1[\"time\"].dt.minute,df1[\"time\"].dt.hour,df1[\"date\"].dt.month, df1[\"date\"].dt.date])['tweet'].count().plot(kind=kind,figsize=(15,15),x ='time',y='number of tweets',title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryp53EIMadjY"
      },
      "source": [
        "df1,df2,df3 = all_dfs['1108723895207628800']['dataframes']\n",
        "ax = plot_activity(df3.copy(deep=True),'reply content by minutes - 1108723895207628800 ')\n",
        "# ax.figure.savefig('/content/drive/My Drive/Graphs/graph5.png')\n",
        "ax"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}